{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef922d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ultralytics import SAM\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from ultralytics.data.utils import visualize_image_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126924a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class folders should be organized as follows:\n",
    "#\n",
    "# raw/\n",
    "# └── screwdriver_kaggle/\n",
    "#    ├── screwdriver/\n",
    "#    │   ├── images/\n",
    "#    │   │   ├── 1.jpg\n",
    "#    │   │   └── ...\n",
    "#    │   ├── labels/\n",
    "#    │   │   ├── 1.txt\n",
    "#    │   │   └── ...\n",
    "#    │   ├── masks/\n",
    "#    │   │   ├── 1_mask.png\n",
    "#    │   │   └── ...  \n",
    "#    ├── hammer/\n",
    "#    │   └── .../\n",
    "#    ├── .../\n",
    "#    └── classes.txt\n",
    "\n",
    "# Darknet files should have the following format:\n",
    "# class_index x_center y_center width height\n",
    "#\n",
    "# e.g.\n",
    "# 0 0.5 0.5 0.2 0.2\n",
    "\n",
    "# classes.txt should contain the class names, one per line:\n",
    "# e.g.\n",
    "# screwdriver\n",
    "# hammer\n",
    "# ...\n",
    "\n",
    "raw_dataset_name = 'kaggle_v0'\n",
    "cnp_output_name = 'cut_and_paste_root2'\n",
    "output_dataset_name = 'cut_and_paste_vik_test'\n",
    "object_classes = ['screwdriver', 'hammer']\n",
    "class_dirs = [f'/home/data/raw/{raw_dataset_name}/{object_class}/' for object_class in object_classes]\n",
    "output_dirs = class_dirs\n",
    "# output_dirs = [f'/home/data/raw/{raw_dataset_name}/{cnp_output_name}/{object_class}/' for object_class in object_classes]\n",
    "# output_dirs = [f'/home/data/raw/{project_name}/{object_class}/' for object_class in object_classes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b13f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images(input_dir, output_dir, size=(640//3, 480//3)):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for img_file in glob.glob(os.path.join(input_dir, '*.jpg')):\n",
    "        img = Image.open(img_file)\n",
    "        img = img.resize(size, Image.LANCZOS)\n",
    "        img.save(os.path.join(output_dir, os.path.basename(img_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae4644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for class_dir in class_dirs:\n",
    "#     resize_images(os.path.join(class_dir, 'images'), os.path.join(class_dir, 'images_resized'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d79286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_darknet_bboxes(bbox_path, image_width, image_height):\n",
    "\t\"\"\"Read bounding boxes from darknet format file and convert to pixel coordinates\"\"\"\n",
    "\tbboxes = []\n",
    "\t\n",
    "\twith open(bbox_path, 'r') as f:\n",
    "\t\tfor line in f:\n",
    "\t\t\tparts = line.strip().split()\n",
    "\t\t\tassert len(parts) == 5, f\"Invalid bbox line: {line.strip()}\"\n",
    "\t\t\t\n",
    "\t\t\t# Darknet format: class_id x_center y_center width height (normalized)\n",
    "\t\t\tclass_id = int(parts[0])\n",
    "\t\t\tx_center = float(parts[1])\n",
    "\t\t\ty_center = float(parts[2])\n",
    "\t\t\twidth = float(parts[3])\n",
    "\t\t\theight = float(parts[4])\n",
    "\t\t\t\n",
    "\t\t\t# Convert from normalized coordinates to pixel coordinates\n",
    "\t\t\tx_center_px = x_center * image_width\n",
    "\t\t\ty_center_px = y_center * image_height\n",
    "\t\t\twidth_px = width * image_width\n",
    "\t\t\theight_px = height * image_height\n",
    "\t\t\t\n",
    "\t\t\t# Convert to x1, y1, x2, y2 format\n",
    "\t\t\tx1 = int(x_center_px - width_px / 2)\n",
    "\t\t\ty1 = int(y_center_px - height_px / 2)\n",
    "\t\t\tx2 = int(x_center_px + width_px / 2)\n",
    "\t\t\ty2 = int(y_center_px + height_px / 2)\n",
    "\t\t\t\n",
    "\t\t\t# Ensure coordinates are within image bounds\n",
    "\t\t\tx1 = max(0, min(x1, image_width - 1))\n",
    "\t\t\ty1 = max(0, min(y1, image_height - 1))\n",
    "\t\t\tx2 = max(0, min(x2, image_width - 1))\n",
    "\t\t\ty2 = max(0, min(y2, image_height - 1))\n",
    "\t\t\t\n",
    "\t\t\tbboxes.append([x1, y1, x2, y2])\n",
    "\n",
    "\treturn bboxes\n",
    "\n",
    "def segment_images_from_folder_bbox(class_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Segments images in the specified folder using the SAM model with bbox information.\n",
    "    Assumes class_dir contains two folders: 'images_resized' and 'labels'.\n",
    "    Each image in 'images_resized' should have a corresponding label file in 'labels' with\n",
    "    bounding box information in the format: x y w h (where x, y are the\n",
    "    top-left corner coordinates and w, h are the width and height of the bounding box).\n",
    "    \"\"\"\n",
    "    model = SAM(\"sam2.1_l.pt\")\n",
    "\n",
    "    for image_path, bbox_path in list(zip(sorted(glob.glob(os.path.join(class_dir, 'images', '*'))), \n",
    "                                          sorted(glob.glob(os.path.join(class_dir, 'labels', '*.txt'))))):\n",
    "        image_dimensions = cv2.imread(image_path).shape\n",
    "        bboxes = read_darknet_bboxes(bbox_path, image_dimensions[1], image_dimensions[0])\n",
    "        \n",
    "        # Predict segmentation using the SAM model with bounding box\n",
    "        results = model(image_path, bboxes=bboxes)\n",
    "        # visualize_image_annotations(image_path, bbox_path, output_dir)\n",
    "        for result in results:\n",
    "            # Access the masks\n",
    "            masks = results[0].masks\n",
    "\n",
    "            # Assuming single class segmentation for simplicity, adjust as needed\n",
    "            mask = masks[0].data.squeeze().cpu().numpy()  # For multi-class, iterate over masks\n",
    "            mask = mask.astype(np.uint8) # Convert mask to uint8 if needed)\n",
    "            mask = cv2.resize(mask, (image_dimensions[1], image_dimensions[0]))\n",
    "            \n",
    "            image = cv2.imread(image_path)\n",
    "            image = cv2.resize(image, (image_dimensions[1], image_dimensions[0]))\n",
    "            \n",
    "            # Negate the mask and mask the image\n",
    "            negative_mask = 1-mask\n",
    "            negative_image = cv2.bitwise_not(image)\n",
    "            negative_image = cv2.bitwise_and(negative_image, negative_image, mask=mask)\n",
    "            masked_image = cv2.bitwise_not(negative_image)\n",
    "            \n",
    "            # Uncomment to see the negated masks\n",
    "            # plt.imshow(negative_mask, cmap='gray')\n",
    "            # plt.axis('off')\n",
    "            # plt.show()\n",
    "\n",
    "            # Uncomment to plot the masked images\n",
    "            # plt.imshow(cv2.cvtColor(masked_image, cv2.COLOR_BGR2RGB))\n",
    "            # plt.axis('off')\n",
    "            # plt.show()\n",
    "\n",
    "            os.mkdir(output_dir) if not os.path.exists(output_dir) else None\n",
    "            for subdir in ['images', 'masks', 'labels']:\n",
    "            # Create subdirectories if they do not exist\n",
    "                subdir_path = os.path.join(output_dir, subdir)\n",
    "                if not os.path.exists(subdir_path): \n",
    "                    os.mkdir(subdir_path)\n",
    "                    \n",
    "            cv2.imwrite(os.path.join(output_dir, 'masks', os.path.basename(image_path).split('.')[0] + '_mask.png'), negative_mask*255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97bbb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists(f'/home/data/raw/{raw_dataset_name}/{cnp_output_name}/'):\n",
    "#     os.mkdir(f'/home/data/raw/{raw_dataset_name}/{cnp_output_name}')\n",
    "\n",
    "# os.system(f\"cp /home/data/raw/{raw_dataset_name}/classes.txt /home/data/raw/{raw_dataset_name}/{cnp_output_name}/\")\n",
    "\n",
    "for class_dir, output_dir in list(zip(class_dirs, output_dirs)):\n",
    "\tsegment_images_from_folder_bbox(class_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33faf33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfd5dd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of background images : 8128\n",
      "List of distractor files collected: []\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/1_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/2_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/4_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/5_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/7_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/8_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/9_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/14_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/11_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/12_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/15_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/16_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/18_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/19_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/21_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/22_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/23_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/25_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/26_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/10_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/28_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/29_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/30_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/3_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/27_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/32_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/6_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/33_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/17_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/35_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/36_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/37_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/34_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/39_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/40_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/42_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/43_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/44_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/46_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/13_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/47_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/31_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/49_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/50_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/51_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/38_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/53_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/54_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/56_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/57_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/58_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/60_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/61_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/63_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/64_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/65_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/67_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/68_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/70_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/71_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/72_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/74_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/41_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/69_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/75_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/59_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/76_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/73_none.jpg\n",
      "Working on /home/data/processed/kaggle_v0/cut_and_paste_vik_test/images/52_none.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "multiprocessing.pool.RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 48, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/home/vikhyat/RIPS25-AnalogDevices-ObjectDetection/src/Cut-and-Paste/dataset_generator.py\", line 160, in create_image_anno_wrapper\n",
      "    return create_image_anno(*args, w=w, h=h, scale_augment=scale_augment, rotation_augment=rotation_augment, blending_list=blending_list, dontocclude=dontocclude)\n",
      "  File \"/home/vikhyat/RIPS25-AnalogDevices-ObjectDetection/src/Cut-and-Paste/dataset_generator.py\", line 240, in create_image_anno\n",
      "    y = random.randint(int(-MAX_TRUNCATION_FRACTION*o_h), int(h-o_h+MAX_TRUNCATION_FRACTION*o_h))\n",
      "  File \"/usr/lib/python3.10/random.py\", line 370, in randint\n",
      "    return self.randrange(a, b+1)\n",
      "  File \"/usr/lib/python3.10/random.py\", line 353, in randrange\n",
      "    raise ValueError(\"empty range for randrange() (%d, %d, %d)\" % (istart, istop, width))\n",
      "ValueError: empty range for randrange() (-1007, -1102, -95)\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vikhyat/RIPS25-AnalogDevices-ObjectDetection/src/Cut-and-Paste/dataset_generator.py\", line 425, in <module>\n",
      "    args = parser.parse_args()\n",
      "  File \"/home/vikhyat/RIPS25-AnalogDevices-ObjectDetection/src/Cut-and-Paste/dataset_generator.py\", line 403, in generate_synthetic_dataset\n",
      "    os.makedirs(img_dir)\n",
      "  File \"/home/vikhyat/RIPS25-AnalogDevices-ObjectDetection/src/Cut-and-Paste/dataset_generator.py\", line 370, in gen_syn_data\n",
      "    p = Pool(NUMBER_OF_WORKERS, init_worker)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 367, in map\n",
      "    return self._map_async(func, iterable, mapstar, chunksize).get()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 774, in get\n",
      "    raise self._value\n",
      "ValueError: empty range for randrange() (-1007, -1102, -95)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(f'python Cut-and-Paste/dataset_generator.py --scale --rotation --num 1 /home/data/raw/{raw_dataset_name}/ /home/data/processed/{raw_dataset_name}/{output_dataset_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d7b0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get image size in bytes\n",
    "print(cv2.imread('/home//data/processed/screwdriver_kaggle/{output_dataset_name}/train/images/1_box.jpg').shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
