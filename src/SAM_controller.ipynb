{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef922d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from ultralytics import SAM\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from ultralytics.data.utils import visualize_image_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126924a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class folders should be organized as follows:\n",
    "#\n",
    "# raw/\n",
    "# └── screwdriver_kaggle/\n",
    "#    ├── screwdriver/\n",
    "#    │   ├── images/\n",
    "#    │   │   ├── 1.jpg\n",
    "#    │   │   └── ...\n",
    "#    │   ├── labels/\n",
    "#    │   │   ├── 1.txt\n",
    "#    │   │   └── ...\n",
    "#    │   ├── masks/\n",
    "#    │   │   ├── 1_mask.png\n",
    "#    │   │   └── ...  \n",
    "#    ├── hammer/\n",
    "#    │   └── .../\n",
    "#    ├── .../\n",
    "#    └── classes.txt\n",
    "\n",
    "# Darknet files should have the following format:\n",
    "# class_index x_center y_center width height\n",
    "#\n",
    "# e.g.\n",
    "# 0 0.5 0.5 0.2 0.2\n",
    "\n",
    "# classes.txt should contain the class names, one per line:\n",
    "# e.g.\n",
    "# screwdriver\n",
    "# hammer\n",
    "# ...\n",
    "\n",
    "raw_dataset_name = 'kaggle_v0'\n",
    "cnp_output_name = 'cut_and_paste_root2'\n",
    "output_dataset_name = 'cut_and_paste_vik_test'\n",
    "object_classes = ['screwdriver', 'hammer', 'plier', 'trowel', 'wrench', 'tape_measure']\n",
    "class_dirs = [f'/home/data/raw/{raw_dataset_name}/{object_class}/' for object_class in object_classes]\n",
    "output_dirs = class_dirs\n",
    "# output_dirs = [f'/home/data/raw/{raw_dataset_name}/{cnp_output_name}/{object_class}/' for object_class in object_classes]\n",
    "# output_dirs = [f'/home/data/raw/{project_name}/{object_class}/' for object_class in object_classes]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674457ef",
   "metadata": {},
   "source": [
    "## Segment w SAM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d79286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_darknet_bboxes(bbox_path, image_width, image_height):\n",
    "\t\"\"\"Read bounding boxes from darknet format file and convert to pixel coordinates\"\"\"\n",
    "\tbboxes = []\n",
    "\t\n",
    "\twith open(bbox_path, 'r') as f:\n",
    "\t\tfor line in f:\n",
    "\t\t\tparts = line.strip().split()\n",
    "\t\t\tassert len(parts) == 5, f\"Invalid bbox line: {line.strip()}\"\n",
    "\t\t\t\n",
    "\t\t\t# Darknet format: class_id x_center y_center width height (normalized)\n",
    "\t\t\tclass_id = int(parts[0])\n",
    "\t\t\tx_center = float(parts[1])\n",
    "\t\t\ty_center = float(parts[2])\n",
    "\t\t\twidth = float(parts[3])\n",
    "\t\t\theight = float(parts[4])\n",
    "\t\t\t\n",
    "\t\t\t# Convert from normalized coordinates to pixel coordinates\n",
    "\t\t\tx_center_px = x_center * image_width\n",
    "\t\t\ty_center_px = y_center * image_height\n",
    "\t\t\twidth_px = width * image_width\n",
    "\t\t\theight_px = height * image_height\n",
    "\t\t\t\n",
    "\t\t\t# Convert to x1, y1, x2, y2 format\n",
    "\t\t\tx1 = int(x_center_px - width_px / 2)\n",
    "\t\t\ty1 = int(y_center_px - height_px / 2)\n",
    "\t\t\tx2 = int(x_center_px + width_px / 2)\n",
    "\t\t\ty2 = int(y_center_px + height_px / 2)\n",
    "\t\t\t\n",
    "\t\t\t# Ensure coordinates are within image bounds\n",
    "\t\t\tx1 = max(0, min(x1, image_width - 1))\n",
    "\t\t\ty1 = max(0, min(y1, image_height - 1))\n",
    "\t\t\tx2 = max(0, min(x2, image_width - 1))\n",
    "\t\t\ty2 = max(0, min(y2, image_height - 1))\n",
    "\t\t\t\n",
    "\t\t\tbboxes.append([x1, y1, x2, y2])\n",
    "\n",
    "\treturn bboxes\n",
    "\n",
    "def segment_images_from_folder_bbox(class_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Segments images in the specified folder using the SAM model with bbox information.\n",
    "    Assumes class_dir contains two folders: 'images_resized' and 'labels'.\n",
    "    Each image in 'images_resized' should have a corresponding label file in 'labels' with\n",
    "    bounding box information in the format: x y w h (where x, y are the\n",
    "    top-left corner coordinates and w, h are the width and height of the bounding box).\n",
    "    \"\"\"\n",
    "    model = SAM(\"sam2.1_l.pt\")\n",
    "\n",
    "    for image_path, bbox_path in list(zip(sorted(glob.glob(os.path.join(class_dir, 'images', '*'))), \n",
    "                                          sorted(glob.glob(os.path.join(class_dir, 'labels', '*.txt'))))):\n",
    "        image_dimensions = cv2.imread(image_path).shape\n",
    "        bboxes = read_darknet_bboxes(bbox_path, image_dimensions[1], image_dimensions[0])\n",
    "        \n",
    "        # Predict segmentation using the SAM model with bounding box\n",
    "        results = model(image_path, bboxes=bboxes)\n",
    "        # visualize_image_annotations(image_path, bbox_path, output_dir)\n",
    "        for result in results:\n",
    "            # Access the masks\n",
    "            masks = results[0].masks\n",
    "\n",
    "            # Assuming single class segmentation for simplicity, adjust as needed\n",
    "            mask = masks[0].data.squeeze().cpu().numpy()  # For multi-class, iterate over masks\n",
    "            mask = mask.astype(np.uint8) # Convert mask to uint8 if needed)\n",
    "            mask = cv2.resize(mask, (image_dimensions[1], image_dimensions[0]))\n",
    "            \n",
    "            image = cv2.imread(image_path)\n",
    "            image = cv2.resize(image, (image_dimensions[1], image_dimensions[0]))\n",
    "            \n",
    "            # Negate the mask and mask the image\n",
    "            negative_mask = 1-mask\n",
    "            negative_image = cv2.bitwise_not(image)\n",
    "            negative_image = cv2.bitwise_and(negative_image, negative_image, mask=mask)\n",
    "            masked_image = cv2.bitwise_not(negative_image)\n",
    "            \n",
    "            # Uncomment to see the negated masks\n",
    "            # plt.imshow(negative_mask, cmap='gray')\n",
    "            # plt.axis('off')\n",
    "            # plt.show()\n",
    "\n",
    "            # Uncomment to plot the masked images\n",
    "            # plt.imshow(cv2.cvtColor(masked_image, cv2.COLOR_BGR2RGB))\n",
    "            # plt.axis('off')\n",
    "            # plt.show()\n",
    "\n",
    "            os.mkdir(output_dir) if not os.path.exists(output_dir) else None\n",
    "            for subdir in ['images', 'masks', 'labels']:\n",
    "            # Create subdirectories if they do not exist\n",
    "                subdir_path = os.path.join(output_dir, subdir)\n",
    "                if not os.path.exists(subdir_path): \n",
    "                    os.mkdir(subdir_path)\n",
    "                    \n",
    "            cv2.imwrite(os.path.join(output_dir, 'masks', os.path.basename(image_path).split('.')[0] + '_mask.png'), negative_mask*255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b13f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images(input_dir, output_dir, size=(640//3, 480//3)):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for img_file in glob.glob(os.path.join(input_dir, '*.jpg')):\n",
    "        img = Image.open(img_file)\n",
    "        img = img.resize(size, Image.LANCZOS)\n",
    "        img.save(os.path.join(output_dir, os.path.basename(img_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae4644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for class_dir in class_dirs:\n",
    "#     resize_images(os.path.join(class_dir, 'images'), os.path.join(class_dir, 'images_resized'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97bbb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists(f'/home/data/raw/{raw_dataset_name}/{cnp_output_name}/'):\n",
    "#     os.mkdir(f'/home/data/raw/{raw_dataset_name}/{cnp_output_name}')\n",
    "\n",
    "# os.system(f\"cp /home/data/raw/{raw_dataset_name}/classes.txt /home/data/raw/{raw_dataset_name}/{cnp_output_name}/\")\n",
    "\n",
    "for class_dir, output_dir in list(zip(class_dirs, output_dirs)):\n",
    "\tsegment_images_from_folder_bbox(class_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2789821",
   "metadata": {},
   "source": [
    "## Test stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91acd46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'python Cut-and-Paste/dataset_generator.py --scale --rotation --num 1 /home/data/raw/{raw_dataset_name}/ /home/data/processed/{raw_dataset_name}/{output_dataset_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd1025fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "raw_dataset_name = 'kaggle_v0'\n",
    "output_dataset_name = 'synthetic-large'\n",
    "\n",
    "do_profile = False\n",
    "profile_addon = \"-m cProfile -o tmp/results.prof \" if do_profile else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e33faf33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python  Cut-and-Paste/dataset_generator.py --scale --rotation --num 100 --dont_occlude_much --dont_parallelize --max_obj_images 20 /home/data/raw/kaggle_v0/ /home/data/processed/kaggle_v0/synthetic-large\n"
     ]
    }
   ],
   "source": [
    "# os.system(f'rm -rf /home/data/processed/{raw_dataset_name}/{output_dataset_name}')\n",
    "cmd = f'python {profile_addon} Cut-and-Paste/dataset_generator.py --scale --rotation --num 100 --dont_occlude_much --dont_parallelize --max_obj_images 20 /home/data/raw/{raw_dataset_name}/ /home/data/processed/{raw_dataset_name}/{output_dataset_name}'\n",
    "print(cmd)\n",
    "\n",
    "# os.system(cmd)\n",
    "# os.system(\"\"\"python -c \"import pstats; p = pstats.Stats('results.prof'); p.sort_stats('cumulative').print_stats()\" > tmp/output.txt\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d1f59d",
   "metadata": {},
   "source": [
    "# Try cv2 Seamless Clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69905842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bg_path = '/home/data/raw/backgrounds/indoorCVPR_09_images/dining057.jpg'\n",
    "bg_path = '/home/data/raw/backgrounds/indoorCVPR_09_images/arton447_ba6a0.jpg'\n",
    "# img_path = '/home/data/raw/kaggle_v0/hammer/images/a7cc2f51-40c3ae9f-1bda-4ace-896e-971d9adddd0e.jpg'\n",
    "img_path ='/home/data/raw/kaggle_v0/screwdriver/images/82d8c17f-43df566b-cdbf-4c7a-b5bb-1f1cb7544c43.jpg'\n",
    "mask_path = img_path.replace('.jpg', '_mask.png').replace('.jpeg', '_mask.png').replace('/images','/masks')\n",
    "# label_path = img_path.replace('.jpg', '.txt').replace('.jpeg', '.txt').replace('/images','/labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b830c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = cv2.imread(bg_path)     # Background image (3‑channel BGR)\n",
    "src = cv2.imread(img_path)     # Foreground image (3‑channel BGR)\n",
    "mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # Single‑channel mask: 255 = foreground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363d7ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = Image.fromarray(dst)\n",
    "src = Image.fromarray(src)\n",
    "mask = Image.fromarray(mask)\n",
    "\n",
    "dst = dst.resize((640*4, 480*4), Image.LANCZOS)\n",
    "new_size = (319, 593)\n",
    "src = src.resize(new_size, Image.LANCZOS)\n",
    "mask = mask.resize(new_size, Image.NEAREST)\n",
    "\n",
    "dst = np.array(dst)\n",
    "src = np.array(src)\n",
    "mask = 255-np.array(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41a53b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given an array of images, show them in a grid (along with their title headings describing their shapes and min max values)\n",
    "def show_images(img_array, figsize=(3,2)):\n",
    "\tn = len(img_array)\n",
    "\tcols = n\n",
    "\tfig, axes = plt.subplots(1, cols, figsize=(figsize[0]*n, figsize[1]))\n",
    "\tif not hasattr(axes, '__iter__'):\n",
    "\t\taxes = [axes]\n",
    "\tfor i, img in enumerate(img_array):\n",
    "\t\tax = axes[i]\n",
    "\t\tax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\t\tax.set_title(f'Shape: {img.shape}, Min: {np.min(img)}, Max: {np.max(img)}, Type: {img.dtype}', fontsize=8)\n",
    "\t\tax.axis('off')\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32136625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show results\n",
    "show_images([dst, src, mask], figsize=(6, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d8640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the location where the center of src will be cloned onto dst\n",
    "# E.g., center = (x, y) pixel coordinates in dst\n",
    "x_frac = 1/2\n",
    "y_frac = 1/2\n",
    "# center = (int(dst.shape[1] * x_frac), int(dst.shape[0] * y_frac))\n",
    "center = (1619, 159)\n",
    "\n",
    "# Perform seamless cloning\n",
    "print(f'Cloning at center {center}')\n",
    "output = cv2.seamlessClone(src, dst, mask.copy(), center, cv2.NORMAL_CLONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85a345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the location where the center of src will be cloned onto dst\n",
    "# E.g., center = (x, y) pixel coordinates in dst\n",
    "x_frac = 2/3\n",
    "y_frac = 1/2\n",
    "center = (int(dst.shape[1] * x_frac), int(dst.shape[0] * y_frac))\n",
    "\n",
    "# Perform seamless cloning\n",
    "print(f'Cloning at center {center}')\n",
    "output2 = cv2.seamlessClone(src, dst, mask.copy(), center, cv2.NORMAL_CLONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9d06d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images([output], figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aca175",
   "metadata": {},
   "source": [
    "# Try other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a707a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tensor = torch.load('test/target_tensor.pt')\n",
    "source_tensor = torch.load('test/source_tensor.pt')\n",
    "mask_tensor = torch.load('test/mask_tensor.pt')\n",
    "loaded_background_array = torch.load('test/background_array.pt')\n",
    "offset_adj = torch.tensor([135, 345])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666bec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_tensor.min(), target_tensor.max(), source_tensor.min(), source_tensor.max(), mask_tensor.min(), mask_tensor.max(), loaded_background_array.min(), loaded_background_array.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba092ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "background_array = pietorch.blend(target_tensor, source_tensor, mask_tensor, offset_adj, False, channels_dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b469c0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save background_array to image\n",
    "# background_array\n",
    "plt.imshow(background_array.numpy())\n",
    "plt.show()\n",
    "plt.imshow((background_array.numpy()*255).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dc16d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_clip = np.clip(loaded_background_array, 0, 1)\n",
    "print(loaded_background_array.max(), loaded_background_array.min(), loaded_background_array.shape)\n",
    "print(load_clip.max(), load_clip.min(), load_clip.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7117bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save background_array to image\n",
    "# background_array\n",
    "plt.imshow(loaded_background_array.numpy())\n",
    "plt.show()\n",
    "plt.imshow(load_clip.numpy())\n",
    "plt.show()\n",
    "plt.imshow((load_clip.numpy()*255).astype(np.uint8))\n",
    "plt.show()\n",
    "plt.imshow((loaded_background_array.numpy()*255).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5cb7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_tensor = TF.to_tensor(synth_images[i]).permute(1,2,0)\n",
    "# source_tensor = TF.to_tensor(foreground).permute(1,2,0)\n",
    "# mask_tensor = TF.to_tensor(mask).squeeze(0) #PIL2array1C(mask)\n",
    "# source_tensor, mask_tensor, offset_adj = trim_mask(target_tensor, source_tensor, mask_tensor, offset)\n",
    "# offset_adj = torch.tensor(offset_adj[::-1])\n",
    "# print(offset_adj)\n",
    "\n",
    "\n",
    "# img_1 = Image.open('/home/data/raw/backgrounds/bg-20k/BG-20k/train/h_0a0c92d7.jpg')\n",
    "# target_tensor_manual = TF.to_tensor(img_1).permute(1,2,0)\n",
    "\n",
    "background_array = blend(target_tensor_manual, source_tensor, mask_tensor, offset_adj, False, channels_dim=2)\n",
    "\n",
    "result = Image.fromarray((background_array.numpy()*255).astype(np.uint8), 'RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c869094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a7698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909ad615",
   "metadata": {},
   "outputs": [],
   "source": [
    "background_array.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b061c5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# background_array -= background_array.min()\n",
    "# background_array /= background_array.max()\n",
    "plt.imshow(background_array.numpy())\n",
    "plt.show()\n",
    "plt.imshow(Image.fromarray((background_array.numpy()*255).astype(np.uint8), 'RGB'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757e9b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PIL2array1C(img):\n",
    "    '''Converts a PIL image to NumPy Array\n",
    "\n",
    "    Args: img(PIL Image): Input PIL image\n",
    "    Returns: NumPy Array: Converted image\n",
    "    '''\n",
    "    return np.array(img.getdata(), np.uint8).reshape(img.size[1], img.size[0])\n",
    "\n",
    "def PIL2array3C(img):\n",
    "    '''Converts a PIL image to NumPy Array\n",
    "\n",
    "    Args: img(PIL Image): Input PIL image\n",
    "    Returns: NumPy Array: Converted image\n",
    "    '''\n",
    "    return np.array(img.getdata(), np.uint8).reshape(img.size[1], img.size[0], 3)\n",
    "\n",
    "def show_details(img, figsize=(5,5)):\n",
    "\tif isinstance(img, Image.Image):\n",
    "\t\tprint(f\"Shape: {img.size} Min: {img.getextrema()[0]}, Max: {img.getextrema()[1]}\")\n",
    "\telse:\n",
    "\t\tprint(f\"Shape: {img.shape} Min: {img.min()}, Max: {img.max()}\")\n",
    "\t\n",
    "\tplt.figure(figsize=figsize)\n",
    "\tplt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71461da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_path = '/home/data/raw/backgrounds/indoorCVPR_09_images/dining057.jpg'\n",
    "img_path = '/home/data/raw/kaggle_v0/hammer/images/a7cc2f51-40c3ae9f-1bda-4ace-896e-971d9adddd0e.jpg'\n",
    "mask_path = img_path.replace('.jpg', '_mask.png').replace('.jpeg', '_mask.png').replace('/images','/masks')\n",
    "label_path = img_path.replace('.jpg', '.txt').replace('.jpeg', '.txt').replace('/images','/labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f2fd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = cv2.imread(bg_path)\n",
    "source = cv2.imread(img_path)\n",
    "mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "print(target.shape, source.shape, mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835f7cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pie.poisson_edit(source, target, mask, offset=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93ab96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# bg_path = '/home/data/raw/backgrounds/indoorCVPR_09_images/arton447_ba6a0.jpg'\n",
    "# bg_path = '/home/data/raw/backgrounds/bg-20k/BG-20k/train/h_0a0c92d7.jpg'\n",
    "\n",
    "target = img_1\n",
    "# img_1 = Image.open(bg_path)\n",
    "# img_1 = img_1.resize((640*4, 480*4), Image.LANCZOS)\n",
    "# target = TF.to_tensor(img_1).transpose(0,2).transpose(0,1)\n",
    "# target = PIL2array3C(img_1)\n",
    "\n",
    "# img_path = '/home/data/raw/kaggle_v0/hammer/images/0a89821a-211e2d2f-45eb-451d-9885-dfc3b07e2bf9.jpg'\n",
    "# img_path = '/home/data/raw/kaggle_v0/screwdriver/images/35f643d3-f6373cbc-ab88-454f-b386-e0843be187b4.jpeg'\n",
    "\n",
    "# source = Image.open(img_path)\n",
    "# source = Image.open(img_path)\n",
    "# resize source image to be smaller\n",
    "scaling_factor = min(target.shape[1], target.shape[0])/max(source.shape[0], source.shape[1]) * 0.9\n",
    "resized_dims = (int(source.shape[0] * scaling_factor), int(source.shape[1] * scaling_factor))\n",
    "source = cv2.resize(source, resized_dims, interpolation=cv2.INTER_LANCZOS4)\n",
    "# source = source.rotate(15, expand=True)\n",
    "# source = TF.to_tensor(source).transpose(0,2).transpose(0,1)\n",
    "# source = PIL2array3C(source)\n",
    "\n",
    "# loaded_mask_tensor = torch.load('test/mask_tensor.pt').numpy()\n",
    "# loaded_mask_img = Image.open('test/mask.jpg')\n",
    "# mask_img = loaded_mask_img\n",
    "# print(mask_img.min(), mask_img.max())\n",
    "\n",
    "# mask_img = Image.fromarray(mask_img)#.convert('1')\n",
    "# mask_img = Image.fromarray(255-PIL2array1C(mask_img)).convert('1')\n",
    "# mask_img = mask_img.resize(resized_dims, Image.LANCZOS)\n",
    "# print(mask_img.size)\n",
    "# mask_img = mask_img.rotate(15, expand=True)\n",
    "# print(mask_img.size)\n",
    "# mask = TF.to_tensor(mask_img).squeeze(0)\n",
    "# mask = (1-mask)  # Invert mask and normalize to [0, 1]\n",
    "# plt.imshow(mask)\n",
    "\n",
    "\n",
    "# mask_img = Image.open(mask_path)\n",
    "# mask_img = Image.fromarray(255-PIL2array1C(mask_img)).convert('1')\n",
    "# # xmin, ymin, xmax, ymax = read_darknet_bboxes(label_path, mask_img.size[0], mask_img.size[1])[0]\n",
    "# # mask_img = mask_img.crop((xmin, ymin, xmax, ymax))\n",
    "mask_img = cv2.resize(mask_img, resized_dims, interpolation=cv2.INTER_LANCZOS4)\n",
    "mask = mask_img\n",
    "\n",
    "# show_details(mask_img)\n",
    "\n",
    "# mask_img = mask_img.rotate(15, expand=True)\n",
    "# mask = TF.to_tensor(mask_img).squeeze(0)\n",
    "# mask = PIL2array1C(mask_img)\n",
    "# TRIMMM\n",
    "# corner = torch.tensor([150,100])\n",
    "corner = (150,100)\n",
    "\n",
    "print('Blending images with shapes:', target.shape, source.shape, mask.shape)\n",
    "\n",
    "center = (corner[0]+source.shape[0]//2, corner[1]+source.shape[1]//2)\n",
    "print('Center:', center)\n",
    "result = cv2.seamlessClone(source, target, 255-mask, center, cv2.NORMAL_CLONE)\n",
    "# result = np.clip(pietorch.blend_wide(target, source, mask, corner, False, channels_dim=2, ), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c56c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_details(loaded_mask_tensor)\n",
    "show_details(target)\n",
    "show_details(source)\n",
    "show_details(mask)\n",
    "show_details(result, figsize=(12,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bc0cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import laplace\n",
    "def poisson_blend(target_img, src_img, mask_img, iter: int = 1024):\n",
    "    for _ in range(iter):\n",
    "        noise = laplace(target_img - src_img)\n",
    "        target_img = target_img + 0.25 * mask_img * noise\n",
    "    return target_img.clip(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9c7d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mask = np.transpose(np.array([mask,mask,mask]), (1,2,0))#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61d9b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = cv2.resize(source, (target.shape[1], target.shape[0]), interpolation=cv2.INTER_LANCZOS4)\n",
    "mask = cv2.resize(mask, (target.shape[1], target.shape[0]), interpolation=cv2.INTER_LANCZOS4)\n",
    "short_result = poisson_blend(target/255, source/255, 1-(new_mask)/255, iter=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4c5828",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_details((short_result*255).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bff21a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29528b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a673089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_conv = (result.numpy()*255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdd8b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imshow the result image in a 12x8 inches figure\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(result_conv)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0476e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.min(), result.max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
