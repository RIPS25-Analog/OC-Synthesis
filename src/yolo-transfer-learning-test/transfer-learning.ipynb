{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b86efdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "572a10b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolo11n.pt\", task='detect', verbose=True)  # Load a pretrained YOLOv8 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47046cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11n summary: 181 layers, 2,624,080 parameters, 0 gradients, 6.6 GFLOPs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(181, 2624080, 0, 6.614336)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0815a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.171 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.167 🚀 Python-3.10.12 torch-2.7.1+cu126 CUDA:0 (NVIDIA A10, 22599MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=coco8.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=2, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=23, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train5, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/vagarwal/RIPS25-AnalogDevices-ObjectDetection/runs/detect/train5, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    464912  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
      "YOLO11n summary: 181 layers, 2,624,080 parameters, 2,624,064 gradients, 6.6 GFLOPs\n",
      "\n",
      "Transferred 499/499 items from pretrained weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvikhyat3\u001b[0m (\u001b[33mvikhyat-3-org\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vagarwal/RIPS25-AnalogDevices-ObjectDetection/src/yolo-transfer-learning-test/wandb/run-20250731_100607-smwx9hd4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vikhyat-3-org/Ultralytics/runs/smwx9hd4' target=\"_blank\">train5</a></strong> to <a href='https://wandb.ai/vikhyat-3-org/Ultralytics' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vikhyat-3-org/Ultralytics' target=\"_blank\">https://wandb.ai/vikhyat-3-org/Ultralytics</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vikhyat-3-org/Ultralytics/runs/smwx9hd4' target=\"_blank\">https://wandb.ai/vikhyat-3-org/Ultralytics/runs/smwx9hd4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing layer 'model.0.conv.weight'\n",
      "Freezing layer 'model.0.bn.weight'\n",
      "Freezing layer 'model.0.bn.bias'\n",
      "Freezing layer 'model.1.conv.weight'\n",
      "Freezing layer 'model.1.bn.weight'\n",
      "Freezing layer 'model.1.bn.bias'\n",
      "Freezing layer 'model.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.3.conv.weight'\n",
      "Freezing layer 'model.3.bn.weight'\n",
      "Freezing layer 'model.3.bn.bias'\n",
      "Freezing layer 'model.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.5.conv.weight'\n",
      "Freezing layer 'model.5.bn.weight'\n",
      "Freezing layer 'model.5.bn.bias'\n",
      "Freezing layer 'model.6.cv1.conv.weight'\n",
      "Freezing layer 'model.6.cv1.bn.weight'\n",
      "Freezing layer 'model.6.cv1.bn.bias'\n",
      "Freezing layer 'model.6.cv2.conv.weight'\n",
      "Freezing layer 'model.6.cv2.bn.weight'\n",
      "Freezing layer 'model.6.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv3.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv3.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv3.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.7.conv.weight'\n",
      "Freezing layer 'model.7.bn.weight'\n",
      "Freezing layer 'model.7.bn.bias'\n",
      "Freezing layer 'model.8.cv1.conv.weight'\n",
      "Freezing layer 'model.8.cv1.bn.weight'\n",
      "Freezing layer 'model.8.cv1.bn.bias'\n",
      "Freezing layer 'model.8.cv2.conv.weight'\n",
      "Freezing layer 'model.8.cv2.bn.weight'\n",
      "Freezing layer 'model.8.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv3.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv3.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv3.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.9.cv1.conv.weight'\n",
      "Freezing layer 'model.9.cv1.bn.weight'\n",
      "Freezing layer 'model.9.cv1.bn.bias'\n",
      "Freezing layer 'model.9.cv2.conv.weight'\n",
      "Freezing layer 'model.9.cv2.bn.weight'\n",
      "Freezing layer 'model.9.cv2.bn.bias'\n",
      "Freezing layer 'model.10.cv1.conv.weight'\n",
      "Freezing layer 'model.10.cv1.bn.weight'\n",
      "Freezing layer 'model.10.cv1.bn.bias'\n",
      "Freezing layer 'model.10.cv2.conv.weight'\n",
      "Freezing layer 'model.10.cv2.bn.weight'\n",
      "Freezing layer 'model.10.cv2.bn.bias'\n",
      "Freezing layer 'model.10.m.0.attn.qkv.conv.weight'\n",
      "Freezing layer 'model.10.m.0.attn.qkv.bn.weight'\n",
      "Freezing layer 'model.10.m.0.attn.qkv.bn.bias'\n",
      "Freezing layer 'model.10.m.0.attn.proj.conv.weight'\n",
      "Freezing layer 'model.10.m.0.attn.proj.bn.weight'\n",
      "Freezing layer 'model.10.m.0.attn.proj.bn.bias'\n",
      "Freezing layer 'model.10.m.0.attn.pe.conv.weight'\n",
      "Freezing layer 'model.10.m.0.attn.pe.bn.weight'\n",
      "Freezing layer 'model.10.m.0.attn.pe.bn.bias'\n",
      "Freezing layer 'model.10.m.0.ffn.0.conv.weight'\n",
      "Freezing layer 'model.10.m.0.ffn.0.bn.weight'\n",
      "Freezing layer 'model.10.m.0.ffn.0.bn.bias'\n",
      "Freezing layer 'model.10.m.0.ffn.1.conv.weight'\n",
      "Freezing layer 'model.10.m.0.ffn.1.bn.weight'\n",
      "Freezing layer 'model.10.m.0.ffn.1.bn.bias'\n",
      "Freezing layer 'model.13.cv1.conv.weight'\n",
      "Freezing layer 'model.13.cv1.bn.weight'\n",
      "Freezing layer 'model.13.cv1.bn.bias'\n",
      "Freezing layer 'model.13.cv2.conv.weight'\n",
      "Freezing layer 'model.13.cv2.bn.weight'\n",
      "Freezing layer 'model.13.cv2.bn.bias'\n",
      "Freezing layer 'model.13.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.13.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.13.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.13.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.13.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.13.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.16.cv1.conv.weight'\n",
      "Freezing layer 'model.16.cv1.bn.weight'\n",
      "Freezing layer 'model.16.cv1.bn.bias'\n",
      "Freezing layer 'model.16.cv2.conv.weight'\n",
      "Freezing layer 'model.16.cv2.bn.weight'\n",
      "Freezing layer 'model.16.cv2.bn.bias'\n",
      "Freezing layer 'model.16.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.16.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.16.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.16.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.16.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.16.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.17.conv.weight'\n",
      "Freezing layer 'model.17.bn.weight'\n",
      "Freezing layer 'model.17.bn.bias'\n",
      "Freezing layer 'model.19.cv1.conv.weight'\n",
      "Freezing layer 'model.19.cv1.bn.weight'\n",
      "Freezing layer 'model.19.cv1.bn.bias'\n",
      "Freezing layer 'model.19.cv2.conv.weight'\n",
      "Freezing layer 'model.19.cv2.bn.weight'\n",
      "Freezing layer 'model.19.cv2.bn.bias'\n",
      "Freezing layer 'model.19.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.19.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.19.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.19.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.19.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.19.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.20.conv.weight'\n",
      "Freezing layer 'model.20.bn.weight'\n",
      "Freezing layer 'model.20.bn.bias'\n",
      "Freezing layer 'model.22.cv1.conv.weight'\n",
      "Freezing layer 'model.22.cv1.bn.weight'\n",
      "Freezing layer 'model.22.cv1.bn.bias'\n",
      "Freezing layer 'model.22.cv2.conv.weight'\n",
      "Freezing layer 'model.22.cv2.bn.weight'\n",
      "Freezing layer 'model.22.cv2.bn.bias'\n",
      "Freezing layer 'model.22.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.22.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.22.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.22.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.22.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.22.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.22.m.0.cv3.conv.weight'\n",
      "Freezing layer 'model.22.m.0.cv3.bn.weight'\n",
      "Freezing layer 'model.22.m.0.cv3.bn.bias'\n",
      "Freezing layer 'model.22.m.0.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.22.m.0.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.22.m.0.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.22.m.0.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.22.m.0.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.22.m.0.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.22.m.0.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.22.m.0.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.22.m.0.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.22.m.0.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.22.m.0.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.22.m.0.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 2445.7±527.8 MB/s, size: 50.0 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/vagarwal/datasets/coco8/labels/train.cache... 4 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 1784.1±990.1 MB/s, size: 54.0 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/vagarwal/datasets/coco8/labels/val.cache... 4 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /home/vagarwal/RIPS25-AnalogDevices-ObjectDetection/runs/detect/train5/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/home/vagarwal/RIPS25-AnalogDevices-ObjectDetection/runs/detect/train5\u001b[0m\n",
      "Starting training for 2 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/2      0.27G      0.598     0.7996      1.081         18        640: 100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  7.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.576       0.85      0.877      0.633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/2     0.307G     0.9697      1.607      1.406         22        640: 100%|██████████| 1/1 [00:00<00:00, 19.53it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 26.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17       0.58       0.85      0.877      0.634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2 epochs completed in 0.000 hours.\n",
      "Optimizer stripped from /home/vagarwal/RIPS25-AnalogDevices-ObjectDetection/runs/detect/train5/weights/last.pt, 5.5MB\n",
      "Optimizer stripped from /home/vagarwal/RIPS25-AnalogDevices-ObjectDetection/runs/detect/train5/weights/best.pt, 5.5MB\n",
      "\n",
      "Validating /home/vagarwal/RIPS25-AnalogDevices-ObjectDetection/runs/detect/train5/weights/best.pt...\n",
      "Ultralytics 8.3.167 🚀 Python-3.10.12 torch-2.7.1+cu126 CUDA:0 (NVIDIA A10, 22599MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 48.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.579       0.85      0.877      0.634\n",
      "Speed: 0.1ms preprocess, 1.6ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1m/home/vagarwal/RIPS25-AnalogDevices-ObjectDetection/runs/detect/train5\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>▁█</td></tr><tr><td>lr/pg1</td><td>▁█</td></tr><tr><td>lr/pg2</td><td>▁█</td></tr><tr><td>metrics/mAP50(B)</td><td>▁█</td></tr><tr><td>metrics/mAP50-95(B)</td><td>▁█</td></tr><tr><td>metrics/precision(B)</td><td>▁█</td></tr><tr><td>metrics/recall(B)</td><td>▁▁</td></tr><tr><td>model/GFLOPs</td><td>▁</td></tr><tr><td>model/parameters</td><td>▁</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>▁</td></tr><tr><td>train/box_loss</td><td>▁█</td></tr><tr><td>train/cls_loss</td><td>▁█</td></tr><tr><td>train/dfl_loss</td><td>▁█</td></tr><tr><td>val/box_loss</td><td>█▁</td></tr><tr><td>val/cls_loss</td><td>█▁</td></tr><tr><td>val/dfl_loss</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>0.0</td></tr><tr><td>lr/pg1</td><td>0.0</td></tr><tr><td>lr/pg2</td><td>0.0</td></tr><tr><td>metrics/mAP50(B)</td><td>0.87705</td></tr><tr><td>metrics/mAP50-95(B)</td><td>0.63351</td></tr><tr><td>metrics/precision(B)</td><td>0.57891</td></tr><tr><td>metrics/recall(B)</td><td>0.85</td></tr><tr><td>model/GFLOPs</td><td>6.614</td></tr><tr><td>model/parameters</td><td>2624080</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>27.326</td></tr><tr><td>train/box_loss</td><td>0.9697</td></tr><tr><td>train/cls_loss</td><td>1.60696</td></tr><tr><td>train/dfl_loss</td><td>1.4063</td></tr><tr><td>val/box_loss</td><td>1.63544</td></tr><tr><td>val/cls_loss</td><td>1.04179</td></tr><tr><td>val/dfl_loss</td><td>1.38079</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">train5</strong> at: <a href='https://wandb.ai/vikhyat-3-org/Ultralytics/runs/smwx9hd4' target=\"_blank\">https://wandb.ai/vikhyat-3-org/Ultralytics/runs/smwx9hd4</a><br> View project at: <a href='https://wandb.ai/vikhyat-3-org/Ultralytics' target=\"_blank\">https://wandb.ai/vikhyat-3-org/Ultralytics</a><br>Synced 5 W&B file(s), 16 media file(s), 10 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250731_100607-smwx9hd4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = model.train(data='coco8.yaml', epochs=2, freeze=23, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ff87f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.167 🚀 Python-3.10.12 torch-2.7.1+cu126 CUDA:0 (NVIDIA A10, 22599MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 2305.4±1000.6 MB/s, size: 54.0 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/vagarwal/datasets/coco8/labels/val.cache... 4 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4/4 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 10.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.578       0.85      0.847      0.632\n",
      "Speed: 0.2ms preprocess, 16.8ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1m/home/vagarwal/RIPS25-AnalogDevices-ObjectDetection/runs/detect/train52\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results_val = model.val(data='coco8.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bafa103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "Class Names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
      "mAP: [    0.27277     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171      0.6965     0.67398     0.63171     0.63171     0.25652     0.63171     0.63171     0.63171     0.63171       0.995\n",
      "     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171\n",
      "     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171      0.8955     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171     0.63171\n",
      "     0.63171     0.63171]\n",
      "Number of Detections per Class: [10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  0  0  2  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Number of Detections per Image: [3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Results Dictionary: {'metrics/precision(B)': np.float64(0.5782468108182871), 'metrics/recall(B)': np.float64(0.85), 'metrics/mAP50(B)': np.float64(0.8472603915344842), 'metrics/mAP50-95(B)': np.float64(0.6317113930126603), 'fitness': np.float64(0.6532662928648427)}\n",
      "Speed: {'preprocess': 0.16591447638347745, 'inference': 16.802283003926277, 'loss': 0.0017559505067765713, 'postprocess': 0.739246781449765}\n"
     ]
    }
   ],
   "source": [
    "# Print the evaluation results\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"Class Names: {results_val.names}\")\n",
    "print(f\"mAP: {results_val.maps}\")\n",
    "print(f\"Number of Detections per Class: {results_val.nt_per_class}\")\n",
    "print(f\"Number of Detections per Image: {results_val.nt_per_image}\")\n",
    "print(f\"Results Dictionary: {results_val.results_dict}\")\n",
    "print(f\"Speed: {results_val.speed}\")\n",
    "\n",
    "result_dict = {'class_names': results_val.names,\n",
    "\t\t\t\t'mAPs': list(map(float, results_val.maps)),\n",
    "\t\t\t\t'nt_per_class': list(map(float, results_val.nt_per_class)),\n",
    "\t\t\t\t'nt_per_image': list(map(float, results_val.nt_per_image)),\n",
    "\t\t\t\t'metrics': {k: float(v) for k, v in results_val.results_dict.items()},\n",
    "\t\t\t\t'speed': results_val.speed,\n",
    "\t\t\t\t# 'model': self.model_path if hasattr(self, 'model_path') else None,\n",
    "\t\t\t\t# 'data': self.val_params.get('data')\n",
    "\t\t\t\t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41317b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metrics/precision(B)': 0.5782468108182871,\n",
       " 'metrics/recall(B)': 0.85,\n",
       " 'metrics/mAP50(B)': 0.8472603915344842,\n",
       " 'metrics/mAP50-95(B)': 0.6317113930126603,\n",
       " 'fitness': 0.6532662928648427}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict['metrics']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
