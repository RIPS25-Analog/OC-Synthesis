{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8688a220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import cv2\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import SAM\n",
    "from huggingface_hub import snapshot_download\n",
    "ROOT_DIR = \"/home/data/pace\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bbc3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download PACE Dataset folders\n",
    "\n",
    "# repo_id = \"qq456cvb/PACE\"  # Replace with the actual dataset ID\n",
    "# local_dir = \".\"  # The local path where you want to save the folder\n",
    "# allow_patterns = [\"model_splits/*\"] # To download a specific folder within the dataset\n",
    "# snapshot_download(repo_id=repo_id, local_dir=local_dir, allow_patterns=allow_patterns, repo_type=\"dataset\")\n",
    "\n",
    "# len(glob.glob(f'{ROOT_DIR}/test/*')), len(glob.glob(f'{ROOT_DIR}/val_inst/*')) ###, len(glob.glob(f'{ROOT_DIR}/val_pbr_cat/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104776f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of all object classes\n",
    "all_categories = ['_'.join(x.split('/')[-1].split('_')[:-1]) for x in glob.glob(f'{ROOT_DIR}/model_splits/category/*_train.txt')]\n",
    "all_categories.sort()\n",
    "print(f\"Total number of unique categories: {len(all_categories)}\")\n",
    "\n",
    "# Compile the data dictionary: data[video_id][frame_id] = set of all instance IDs in that frame of that video\n",
    "data = dict()\n",
    "\n",
    "for video_path in glob.glob(f'{ROOT_DIR}/test/*')+glob.glob(f'{ROOT_DIR}/val_inst/*'):\n",
    "\tvideo_id = int(video_path.split('/')[-1])\n",
    "\tframe_ids = [int(f.split('/')[-1].split('.')[0]) for f in glob.glob(f'{video_path}/rgb/*')]\n",
    "\tdata[video_id] = {frame_id: set() for frame_id in frame_ids}\n",
    "\t\n",
    "\twith open(f'{video_path}/scene_gt_coco_det_modal_inst.json', 'r') as f:\n",
    "\t\tvideo_json = json.load(f)\n",
    "\n",
    "\tfor anno in video_json['annotations']:\n",
    "\t\tobject_instance_id = anno['category_id']\n",
    "\t\tframe_id = anno['image_id']\n",
    "\t\tdata[video_id][frame_id].add(object_instance_id)\n",
    "\n",
    "# Create dictionary to fetch all instance IDs for a given category\n",
    "instances_of_category = dict()\n",
    "for category in all_categories:\n",
    "\ttest_instance_ids = [int(x.split('_')[-1]) for x in open(f'{ROOT_DIR}/model_splits/category/{category}_test.txt', 'r').read().splitlines()]\n",
    "\tval_instance_ids = [int(x.split('_')[-1]) for x in open(f'{ROOT_DIR}/model_splits/category/{category}_val.txt', 'r').read().splitlines()]\n",
    "\tinstances_of_category[category] = set(test_instance_ids + val_instance_ids)\n",
    "\tprint(f'Category {category}, has {len(instances_of_category[category])} instances: {instances_of_category[category]}')\n",
    "\n",
    "# Create set of all instance IDs\n",
    "all_instances = set()\n",
    "for cat in instances_of_category:\n",
    "\tall_instances.update(instances_of_category[cat])\n",
    "print(f'Total number of unique instances: {len(all_instances)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c466e528",
   "metadata": {},
   "source": [
    "# PACE Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a71117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of number of frames per video\n",
    "video_lengths = [len(frames) for frames in data.values()]\n",
    "print(f'Total number of frames: {sum(video_lengths)}')\n",
    "plt.hist(video_lengths, bins=50)\n",
    "plt.xlabel('Number of frames')\n",
    "plt.ylabel('Number of videos')\n",
    "plt.title('Distribution of number of frames per video')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d91699",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_category = 'toys'\n",
    "chosen_instances = instances_of_category[chosen_category]\n",
    "\n",
    "# for each video, count how many different chosen instances show up at all (even if not in every frame)\n",
    "chosen_counts = defaultdict(int)\n",
    "for video_id, frames in data.items():\n",
    "\tfor chosen_instance in chosen_instances:\n",
    "\t\tif any(chosen_instance in frames[frame_id] for frame_id in frames):\n",
    "\t\t\tchosen_counts[video_id] += 1\n",
    "\n",
    "plt.hist(list(chosen_counts.values()), align='mid')\n",
    "plt.xlabel(f'Number of instances of {chosen_category} appearing in a video')\n",
    "plt.ylabel('Number of videos')\n",
    "plt.title(f'Distribution of number of videos containing {chosen_category}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7626aad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each object instance, plot how many videos it appears in (even if it appears in only one frame)\n",
    "# plot object instance ID on the x axis, and number of videos it appears in on the y axis\n",
    "# sort from most to least number of videos\n",
    "instance_video_count = {instance_id: 0 for instance_id in instances_of_category[chosen_category]}\n",
    "for instance_id in instance_video_count:\n",
    "\tfor video_id, frames in data.items():\n",
    "\t\tif any(instance_id in frames[frame_id] for frame_id in frames):\n",
    "\t\t\tprint(f\"Found instance ID: {instance_id} in video ID: {video_id}\")\n",
    "\t\t\tinstance_video_count[instance_id] += 1\n",
    "\n",
    "# sort the bars by number of videos\n",
    "# instance_video_count = dict(sorted(instance_video_count.items(), key=lambda item: item[1], reverse=True))\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(np.arange(len(instance_video_count.keys())), instance_video_count.values())\n",
    "plt.xlabel('Object Instance ID')\n",
    "plt.ylabel('Number of Videos')\n",
    "plt.title('Number of Videos per Object Instance')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb2a279",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make histogram of instaces per category\n",
    "instances_counts = [len(instances_of_category[category]) for category in all_categories]\n",
    "\n",
    "# sort categories by number of instances\n",
    "sorted_categories = [x for _, x in sorted(zip(instances_counts, all_categories), reverse=True)]\n",
    "sorted_counts = sorted(instances_counts, reverse=True)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(sorted_categories, sorted_counts)\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Number of Instances')\n",
    "plt.title(f'Number of Instances per Category')\n",
    "plt.xticks(rotation=90)\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9ea2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot number of videos that each category occurs in\n",
    "category_video_counts = defaultdict(int)\n",
    "for category in all_categories:\n",
    "\tfor video_id, frames in data.items():\n",
    "\t\tfoundCategoryInVideo = False\n",
    "\t\tfor frame_id, object_instances in frames.items():\n",
    "\t\t\tfor instance_id in object_instances:\n",
    "\t\t\t\tif instance_id in instances_of_category[category]:\n",
    "\t\t\t\t\tcategory_video_counts[category] += 1\n",
    "\t\t\t\t\tfoundCategoryInVideo = True\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\tif foundCategoryInVideo:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "# plot number of videos that each category occurs in\n",
    "sorted_categories = [x for _, x in sorted(zip(category_video_counts.values(), category_video_counts.keys()), reverse=True)]\n",
    "sorted_counts = sorted(category_video_counts.values(), reverse=True)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(sorted_categories, sorted_counts)\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Number of Videos')\n",
    "plt.title(f'Number of Videos per Category')\n",
    "plt.xticks(rotation=90)\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b377c73f",
   "metadata": {},
   "source": [
    "# Instance & video train-val-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9493eeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'{ROOT_DIR}/toycar_can_v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef456756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isInstanceInVideo(instance_id, video_id):\n",
    "\t\"\"\"Check if a given instance ID is present in any frame of the specified video.\"\"\"\n",
    "\treturn any(instance_id in frame_instances for frame_instances in data[video_id].values())\n",
    "\n",
    "def set_union(sets):\n",
    "    return set().union(*sets)\n",
    "\n",
    "def sets_are_disjoint(sets):\n",
    "\t\"\"\"Check if all sets in the list are disjoint.\"\"\"\n",
    "\tcombined = set()\n",
    "\tfor s in sets:\n",
    "\t\tif not combined.isdisjoint(s):\n",
    "\t\t\treturn False\n",
    "\t\tcombined.update(s)\n",
    "\treturn True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18534874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for each chosen instance, find all videos that contain it, and all other instances that appear in those videos\n",
    "# video_groups = []\n",
    "# chosen_category = 'toothbrush'\n",
    "# chosen_instances = instances_of_category[chosen_category]\n",
    "# print(chosen_instances)\n",
    "\n",
    "# for chosen_instance in chosen_instances:\n",
    "# \t# check if the chosen instance is already in a group\n",
    "# \tfor group in video_groups:\n",
    "# \t\tif chosen_instance in group['instances']:\n",
    "# \t\t\tbreak\n",
    "# \telse:\n",
    "# \t\t# if not, create a new group \n",
    "# \t\tnew_group = {'video_ids': set(), 'instances': set()}\n",
    "# \t\tfor video_id in data.keys():\n",
    "# \t\t\tif isInstanceInVideo(chosen_instance, video_id):\n",
    "# \t\t\t\tnew_group['video_ids'].add(video_id)\n",
    "# \t\t\t\t# find all other instances in the same video\n",
    "# \t\t\t\tfor other_instance in instances_of_category[chosen_category]:\n",
    "# \t\t\t\t\tif (other_instance not in new_group['instances']) and isInstanceInVideo(other_instance, video_id):\n",
    "# \t\t\t\t\t\tnew_group['instances'].add(other_instance)\n",
    "# \t\t\t\t\t\tfor video_id in data.keys():\n",
    "# \t\t\t\t\t\t\tif isInstanceInVideo(other_instance, video_id):\n",
    "# \t\t\t\t\t\t\t\t# print(f'Adding instance {other_instance} to group for video {video_id}')\n",
    "# \t\t\t\t\t\t\t\tnew_group['video_ids'].add(video_id)\n",
    "# \t\tif new_group['video_ids']:\n",
    "# \t\t\tvideo_groups.append(new_group)\n",
    "# \t\telse:\n",
    "# \t\t\tprint(f'No videos found for instance {chosen_instance} in category {chosen_category}.')\n",
    "\t\t\n",
    "\n",
    "# for group in video_groups:\n",
    "# \ttotal_frames = sum(len(data[video_id]) for video_id in group[\"video_ids\"])\n",
    "# \t# print('---')\n",
    "# \tprint(f'Video group with {len(group[\"video_ids\"])} videos, {len(group[\"instances\"])} instances, and {total_frames} frames.', end=' ')\n",
    "# \t# print(f'Video IDs: {sorted(list(group[\"video_ids\"]))}')\n",
    "# \tprint(f'Instances: {sorted(list(group[\"instances\"]))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a3120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instance_splits['train']['distractor'] = {320, 324, 336, 340,    2, 16,    437, 448}\n",
    "# instance_splits['val']['distractor'] = {315, 316, 317, 318,    5, 6,    434, 435}\n",
    "# instance_splits['test']['distractor'] = {305, 306, 307, 308,    21, 24,   451, 436}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cad1b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_splits = {'train': dict(), 'val': dict(), 'test': dict()}\n",
    "instance_splits = {'train': dict(), 'val': dict(), 'test': dict()}\n",
    "distractor_splits = {'train': dict(), 'val': dict(), 'test': dict()}\n",
    "\n",
    "target_categories = ['toy_car', 'can']\n",
    "\n",
    "instance_splits['train']['toy_car'] = {456, 458, 461, 470}\n",
    "instance_splits['val']['toy_car'] = {459, 460, 467, 468}\n",
    "instance_splits['test']['toy_car'] = {455, 457, 469}\n",
    "\n",
    "instance_splits['train']['can'] = {74, 57, 58}\n",
    "instance_splits['val']['can'] = {66, 70, 71, 73}\n",
    "instance_splits['test']['can'] = {61, 62, 63}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebcb512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the instance splits, construct video splits (all videos containing train instances become train videos, etc.)\n",
    "already_assigned_videos = set()\n",
    "for split in instance_splits:\n",
    "\tfor category in instance_splits[split]:\n",
    "\t\tvideo_splits[split][category] = set()\n",
    "\t\tfor instance in instance_splits[split][category]:\n",
    "\t\t\t# add each video that contains this instance to the split\n",
    "\t\t\tfor video_id, frames in data.items():\n",
    "\t\t\t\tif isInstanceInVideo(instance, video_id):\n",
    "\t\t\t\t\t# if the video is already assigned to another category, this may create a conflict\n",
    "\t\t\t\t\tif (video_id in already_assigned_videos) and (video_id not in video_splits[split][category]):\n",
    "\t\t\t\t\t\tif category == 'distractor':\n",
    "\t\t\t\t\t\t\tcontinue  # Just don't add the distractor videos that would create conflict\n",
    "\t\t\t\t\t\traise ValueError(\"Splits are not disjoint!\")\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tvideo_splits[split][category].add(video_id)\n",
    "\t\t\t\t\talready_assigned_videos.add(video_id)\n",
    "\n",
    "for split in video_splits:\n",
    "\tfor category in video_splits[split]:\n",
    "\t\tprint(f'{split:>5} | {category:>7} | Videos:    {sorted(list(video_splits[split][category]))}')\n",
    "\t\tif len(video_splits[split][category]) == 0:\n",
    "\t\t\tprint(f'Warning: No videos found for split {split} and category {category}.')\n",
    "\n",
    "for split in video_splits:\n",
    "\tfor category in video_splits[split]:\n",
    "\t\tprint(f'{split:>5} | {category:>7} | Instances: {sorted(list(instance_splits[split][category]))}')\n",
    "\t\tif len(instance_splits[split][category]) == 0:\n",
    "\t\t\tprint(f'Warning: No instances found for split {split} and category {category}.')\n",
    "\n",
    "videos_in_data_split = {'train': set(), 'val': set(), 'test': set()} # set of video IDs for each data split\n",
    "instances_in_data_split = {'train': set(), 'val': set(), 'test': set()} # set of TARGET instance IDs for each data split\n",
    "for split in video_splits:\n",
    "\tvideos_in_data_split[split] = set_union(video_splits[split][category] for category in video_splits[split])\n",
    "\tinstances_in_data_split[split] = set_union(instance_splits[split][category] for category in instance_splits[split] if category!='distractor')\n",
    "\n",
    "videos_in_category = {category:set() for category in video_splits['train']} # set of video IDs for each category\n",
    "instances_in_category = {category:set() for category in instance_splits['train']} # set of instance IDs for each category\n",
    "for category in video_splits[split]:\n",
    "\tvideos_in_category[category] = videos_in_category[category].union(video_splits[split][category])\n",
    "\tinstances_in_category[category] = instances_in_category[category].union(instance_splits[split][category])\n",
    "\n",
    "assert sets_are_disjoint([videos_in_data_split['train'], videos_in_data_split['test'], videos_in_data_split['val']])\n",
    "assert sets_are_disjoint([instances_in_data_split['train'], instances_in_data_split['val'], instances_in_data_split['test']])\n",
    "\n",
    "print(f'Train videos: {len(videos_in_data_split[\"train\"])}, Train instances: {len(instances_in_data_split[\"train\"])}')\n",
    "print(f'Val videos: {len(videos_in_data_split[\"val\"])}, Val instances: {len(instances_in_data_split[\"val\"])}')\n",
    "print(f'Test videos: {len(videos_in_data_split[\"test\"])}, Test instances: {len(instances_in_data_split[\"test\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415db83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a dictionary where the key is an instance ID, and the value is a list of video IDs where it appears\n",
    "videos_containing_instance = defaultdict(set)\n",
    "for video_id in data.keys():\n",
    "\tfor instance in all_instances:\n",
    "\t\tif isInstanceInVideo(instance, video_id):\n",
    "\t\t\tvideos_containing_instance[instance].add(video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd399e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find distractor instances that appear in the train videos but not in the val or test videos\n",
    "train_instances = set()\n",
    "# for video_id in videos_in_data_split['train']:\n",
    "for instance_id in videos_containing_instance.keys():  # Check the first frame of the video\n",
    "\t# if any(video_id in videos_in_data_split['train'] for video_id in videos_containing_instance[instance_id]) and \\\n",
    "\n",
    "\tif any(video_id in videos_in_data_split['train'] for video_id in videos_containing_instance[instance_id]) and \\\n",
    "\t   not any(video_id in videos_in_data_split['val'] for video_id in videos_containing_instance[instance_id]) and \\\n",
    "\t   not any(video_id in videos_in_data_split['test'] for video_id in videos_containing_instance[instance_id]):\n",
    "\t\ttrain_instances.add(instance_id)\n",
    "\n",
    "target_train_instances = set_union([instance_splits['train'][category] for category in target_categories])\n",
    "distractor_instances = train_instances.difference(target_train_instances)\n",
    "print(f'{len(distractor_instances)} distractor instances: {sorted(list(distractor_instances))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5428458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bbox_to_yolo(bbox, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Convert bounding box from [x, y, width, height] format to YOLO format\n",
    "    [x_center, y_center, width, height] normalized by image dimensions\n",
    "    \"\"\"\n",
    "    x, y, w, h = bbox\n",
    "    x_center = (x + w/2) / img_width\n",
    "    y_center = (y + h/2) / img_height\n",
    "    norm_width = w / img_width\n",
    "    norm_height = h / img_height\n",
    "    return x_center, y_center, norm_width, norm_height\n",
    "\n",
    "def process_annotation_file(json_file_path):\n",
    "    \"\"\"\n",
    "    Process a video annotation file and convert to YOLO format\n",
    "    Returns a dictionary mapping frame_id to list of YOLO format annotations\n",
    "    \"\"\"\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        annotations = json.load(f)['annotations']\n",
    "    \n",
    "    frame_annotations = defaultdict(list)\n",
    "    \n",
    "    for anno in annotations:\n",
    "        # if anno.get('ignore', False):  # Skip ignored annotations\n",
    "        #     print(f\"Skipping ignored annotation: {anno}\")\n",
    "        #     if anno['category_id'] in wallet_instances.union(can_instances):\n",
    "        #         print('can or wallet skipped')\n",
    "        #     continue\n",
    "            \n",
    "        frame_id = anno['image_id']\n",
    "        class_id = anno['category_id']\n",
    "        bbox = anno['bbox']  # [x, y, width, height]\n",
    "        img_width = anno['width']\n",
    "        img_height = anno['height']\n",
    "        \n",
    "        # Convert to YOLO format\n",
    "        x_center, y_center, norm_width, norm_height = convert_bbox_to_yolo(bbox, img_width, img_height)\n",
    "        \n",
    "        yolo_annotation = {\n",
    "            'class_id': class_id,\n",
    "            'x_center': x_center,\n",
    "            'y_center': y_center,\n",
    "            'width': norm_width,\n",
    "            'height': norm_height\n",
    "        }\n",
    "        \n",
    "        frame_annotations[frame_id].append(yolo_annotation)\n",
    "    \n",
    "    return frame_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccabf531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy_car : 0\n",
    "# can : 1\n",
    "category_to_label = {category: i for i, category in enumerate(target_categories)}\n",
    "label_to_category_name = {x: category for category, x in category_to_label.items()} # maps class numbers (from  darknet labels) to category/class names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10b2a6d",
   "metadata": {},
   "source": [
    "# Dataset Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc65e50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_name(instance_id):\n",
    "\tcategory_name_search = [cat for cat in instances_of_category if instance_id in instances_of_category[cat]]\n",
    "\tassert len(category_name_search) >= 0, f\"No categories found for label {instance_id}\"\n",
    "\tassert len(category_name_search) == 1, f\"Multiple categories found for label {instance_id}\"\n",
    "\treturn category_name_search[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c82a577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train, val, test splits\n",
    "for split in ['train', 'val', 'test']:\n",
    "\tif not os.path.exists(f'{output_dir}/{split}/images/'):\n",
    "\t\tos.makedirs(f'{output_dir}/{split}/images/')\n",
    "\tif not os.path.exists(f'{output_dir}/{split}/labels/'):\n",
    "\t\tos.makedirs(f'{output_dir}/{split}/labels/')\n",
    "\tif not os.path.exists(f'{output_dir}-extra/{split}/full_labels/'):\n",
    "\t\tos.makedirs(f'{output_dir}-extra/{split}/full_labels/')\n",
    "\n",
    "for video_path in glob.glob(f'{ROOT_DIR}/test/*')+glob.glob(f'{ROOT_DIR}/val_inst/*'):\n",
    "\tvideo_id = int(video_path.split('/')[-1])\n",
    "\tif video_id in videos_in_data_split['train']:\n",
    "\t\tsplit = 'train'\n",
    "\telif video_id in videos_in_data_split['val']:\n",
    "\t\tsplit = 'val'\n",
    "\telif video_id in videos_in_data_split['test']:\n",
    "\t\tsplit = 'test'\n",
    "\telse:\n",
    "\t\tcontinue\n",
    "\n",
    "\tlabels = process_annotation_file(f'{video_path}/scene_gt_coco_det_modal_inst.json')\n",
    "\tfor frame_id in glob.glob(f'{video_path}/rgb/*'):\n",
    "\t\t# print(f\"cp {frame_id} {ROOT_DIR}/wallet_can_v0/{split}/{str(video_id)}/{video_id}_{os.path.basename(frame_id)}\")\n",
    "\t\tos.system(f\"cp {frame_id} {output_dir}/{split}/images/{str(video_id)}_{int(os.path.basename(frame_id).split('.')[0])}.png\")\n",
    "\n",
    "\t\tframe_num = int(os.path.basename(frame_id).split('.')[0])\n",
    "\t\tfull_label_path = f\"{output_dir}-extra/{split}/full_labels/{str(video_id)}_{frame_num}.txt\"\n",
    "\t\tlabel_path = f\"{output_dir}/{split}/labels/{str(video_id)}_{frame_num}.txt\"\n",
    "\n",
    "\t\tfor label in labels[frame_num]:\n",
    "\t\t\tcategory_name = get_category_name(label['class_id'])\n",
    "\t\t\t\n",
    "\t\t\tif category_name in target_categories:\n",
    "\t\t\t\tcategory_id = category_to_label[category_name]\n",
    "\t\t\t\t# if the category is one of the chosen categories, write to the label file\n",
    "\t\t\t\twith open(label_path, 'a') as f:\n",
    "\t\t\t\t\tf.write(f\"{category_id} {label['x_center']} {label['y_center']} {label['width']} {label['height']}\\n\")\n",
    "\t\t\t\n",
    "\t\t\t# write to the full label file regardless of category\n",
    "\t\t\t### NOTE: store instance IDs here instead of class IDs\n",
    "\t\t\twith open(full_label_path, 'a') as f:\n",
    "\t\t\t\tf.write(f\"{label['class_id']} {label['x_center']} {label['y_center']} {label['width']} {label['height']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef05ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in ['train', 'val', 'test']:\n",
    "    for image_path in glob.glob(f'{output_dir}/{split}/images/*'):\n",
    "        label_path = image_path.replace('.png', '.txt').replace('/images', '/labels')\n",
    "        full_label_path = image_path.replace(f'{output_dir}/', f'{output_dir}-extra/').replace('.png', '.txt').replace('/images', '/full_labels')\n",
    "        if not os.path.exists(label_path):\n",
    "            os.system(f\"touch {label_path}\")\n",
    "        if not os.path.exists(full_label_path):\n",
    "            os.system(f\"touch {full_label_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6435b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in ['train', 'val', 'test']:\n",
    "\tpath = f\"{output_dir}/{split}\"\n",
    "\tfor image_path in glob.glob(f'{path}/images/*'):\n",
    "\t\tif not os.path.exists(image_path.replace('.png', '.txt').replace('/images', '/labels')):\n",
    "\t\t\tprint(f\"Missing label for image: {image_path}\")\n",
    "\n",
    "# for label_path in glob.glob(f'{path}/labels/*'):\n",
    "# \tif not os.path.exists(label_path.replace('.txt', '.png').replace('/labels', '/images')):\n",
    "# \t\tprint(f\"Missing image for label file: {label_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5df239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of images and labels in each split\n",
    "for split in ['train', 'val', 'test']:\n",
    "\tnum_images = len(glob.glob(f'{output_dir}/{split}/images/*'))\n",
    "\tnum_labels = len(glob.glob(f'{output_dir}/{split}/labels/*'))\n",
    "\tnum_full_labels = len(glob.glob(f'{output_dir}-extra/{split}/full_labels/*'))\n",
    "\tprint(f\"{split.capitalize()} - Images: {num_images}, Labels: {num_labels}, Full Labels: {num_full_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64b7d0c",
   "metadata": {},
   "source": [
    "# Segment to get foreground object images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7c3e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_darknet_bboxes(bbox_path, image_width, image_height):\n",
    "\t\"\"\"Read bounding boxes from darknet format file and convert to pixel coordinates\"\"\"\n",
    "\tbboxes = []\n",
    "\t\n",
    "\twith open(bbox_path, 'r') as f:\n",
    "\t\tfoundLine = False\n",
    "\t\tclass_ids = []\n",
    "\t\tfor line in f:\n",
    "\t\t\tfoundLine = True\n",
    "\t\t\tparts = line.strip().split()\n",
    "\t\t\tassert len(parts) == 5, f\"Invalid bbox line: {line.strip()}\"\n",
    "\t\t\t\n",
    "\t\t\t# Darknet format: class_id x_center y_center width height (normalized)\n",
    "\t\t\tclass_id = int(parts[0])\n",
    "\t\t\tclass_ids.append(class_id)\n",
    "\n",
    "\t\t\tx_center, y_center, width, height = map(float, parts[1:5])\n",
    "\n",
    "\t\t\t# Convert from normalized coordinates to pixel coordinates\n",
    "\t\t\tx_center_px = x_center * image_width\n",
    "\t\t\ty_center_px = y_center * image_height\n",
    "\t\t\twidth_px = width * image_width\n",
    "\t\t\theight_px = height * image_height\n",
    "\t\t\t\n",
    "\t\t\t# Convert to x1, y1, x2, y2 format\n",
    "\t\t\tx1 = int(x_center_px - width_px / 2)\n",
    "\t\t\ty1 = int(y_center_px - height_px / 2)\n",
    "\t\t\tx2 = int(x_center_px + width_px / 2)\n",
    "\t\t\ty2 = int(y_center_px + height_px / 2)\n",
    "\t\t\t\n",
    "\t\t\t# Ensure coordinates are within image bounds\n",
    "\t\t\tx1 = max(0, min(x1, image_width - 1))\n",
    "\t\t\ty1 = max(0, min(y1, image_height - 1))\n",
    "\t\t\tx2 = max(0, min(x2, image_width - 1))\n",
    "\t\t\ty2 = max(0, min(y2, image_height - 1))\n",
    "\t\t\t\n",
    "\t\t\tbboxes.append([x1, y1, x2, y2])\n",
    "\t\tif not foundLine:\n",
    "\t\t\tprint(f\"No bounding boxes found in {bbox_path}. Returning empty list.\")\n",
    "\treturn class_ids, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1790188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_images_from_folder_bbox(img_dir, label_dir, output_dir, frame_skip):\n",
    "\t\"\"\"\n",
    "\tSegments images in the given img_dir using the SAM model with bbox information from the given label_dir.\n",
    "\tEach image in img_dir should have a corresponding label file in label_dir (with the same name) with\n",
    "\tbounding box information in the format: class_id x_center y_center width height\n",
    "\t\"\"\"\n",
    "\tos.mkdir(output_dir) if not os.path.exists(output_dir) else None\n",
    "\tsubdir_path = os.path.join(output_dir, 'masks')\n",
    "\tif not os.path.exists(subdir_path): \n",
    "\t\tos.mkdir(subdir_path)\n",
    "\n",
    "\tfor image_path in glob.glob(os.path.join(img_dir, '*')):\n",
    "\t\timage_name = os.path.basename(image_path).split('.')[0]\n",
    "\t\tbbox_path = os.path.join(label_dir, image_name + '.txt')\n",
    "\t\tvideo_id, frame_id = map(int, image_name.split('_')[:2])\n",
    "\t\t\n",
    "\t\tif frame_id % frame_skip != 0:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tprint(f\"Processing video {video_id}, frame {frame_id} from {image_path}\")\n",
    "\t\t\n",
    "\t\timage_dimensions = cv2.imread(image_path).shape\n",
    "\t\tclass_ids, bboxes = read_darknet_bboxes(bbox_path, image_dimensions[1], image_dimensions[0])\n",
    "\t\tif len(set(class_ids)) != len(class_ids):\n",
    "\t\t\tprint(f\"WARNING: Duplicate class IDs found in {bbox_path}: {class_ids}\")\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tif len(bboxes) == 0:\n",
    "\t\t\tprint(f\"No bounding boxes found for {image_path}. Skipping.\")\n",
    "\t\t\tcontinue\n",
    "\t\t\n",
    "\t\t# Predict segmentation using the SAM model with bounding box\n",
    "\t\tresults = model(image_path, bboxes=bboxes)[0]\n",
    "\t\t# visualize_image_annotations(image_path, bbox_path, output_dir)\n",
    "\t\t\n",
    "\t\tfor class_id, mask in zip(class_ids, results.masks):\n",
    "\t\t\t# Assuming single class segmentation for simplicity, adjust as needed\n",
    "\t\t\tmask = mask.data.squeeze().cpu().numpy()  # For multi-class, iterate over masks\n",
    "\n",
    "\t\t\tmask = mask.astype(np.uint8) # Convert mask to uint8 if needed\n",
    "\t\t\tnegative_mask = 1 - mask\n",
    "\n",
    "\t\t\tcv2.imwrite(os.path.join(output_dir, 'masks', os.path.basename(image_path).split('.')[0] + f'_mask_{class_id}.png'), negative_mask*255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf90be14",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_skip = 70\n",
    "model = SAM(\"sam2.1_l.pt\")\n",
    "\n",
    "# for class_dir in glob.glob(os.path.join(output_dir, '*')):\n",
    "img_dir = f'{ROOT_DIR}/toycar_can_v2/train/images'\n",
    "label_dir = f'{ROOT_DIR}/toycar_can_v2-extra/train/full_labels'\n",
    "mask_output_dir = f'{ROOT_DIR}/toycar_can_v2-SAMTEST/train'\n",
    "segment_images_from_folder_bbox(img_dir, label_dir, mask_output_dir, frame_skip=frame_skip)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
